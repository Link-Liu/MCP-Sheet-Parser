# benchmark.py
# æ€§èƒ½åŸºå‡†æµ‹è¯•æ¨¡å—

import argparse
from typing import Dict, Any

from .utils import setup_logger


class BenchmarkRunner:
    """åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.logger = setup_logger(__name__)
    
    def run_benchmark(self, input_file: str) -> int:
        """
        è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            
        Returns:
            int: ç¨‹åºé€€å‡ºä»£ç  (0=æˆåŠŸ, 1=å¤±è´¥)
        """
        try:
            print("ğŸš€ æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
            from .performance import PerformanceBenchmark
            
            benchmark = PerformanceBenchmark()
            results = benchmark.run_comprehensive_benchmark(input_file)
            
            print("\nğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ:")
            for metric, value in results.items():
                print(f"  {metric}: {value}")
            
            return 0
            
        except Exception as e:
            print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            self.logger.error(f"åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return 1


# åˆ›å»ºå…¨å±€åŸºå‡†æµ‹è¯•è¿è¡Œå™¨å®ä¾‹
benchmark_runner = BenchmarkRunner()

# å¯¼å‡ºä¸»è¦å‡½æ•°ä¾›main.pyä½¿ç”¨
run_benchmark = benchmark_runner.run_benchmark 